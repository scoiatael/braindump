#+HUGO_BASE_DIR: ../
#+HUGO_SECTION: ./

* Projects :@projects:
   :PROPERTIES:
   :EXPORT_HUGO_SECTION: projects
   :END:
** DONE Blog                                            :blog:emacs:hugo:org:
   CLOSED: [2020-01-05 Sun 19:47]
   :PROPERTIES:
   :EXPORT_FILE_NAME: blog-start
   :END:
*** What?
    Decided to start a blog.
    Nothing fancy, more like a braindump.
    All the ideas for projects which are currently in various TODO programs (Google Keep, Todoiost, Wunderlist) and org files, some descriptions for already writen projects with links.
    Maybe some ramblings around random thoughts.
*** Why?
    Of all the weird places I could get motivation from, the thought occured when reading RFC. A pretty special one, for that: [[https://tools.ietf.org/html/rfc8700][RFC 8700 - Fifty Years of RFCs]]. At one point author remarked a need for less formalized form of discussion; and by that defined RFC as
    #+BEGIN_QUOTE
    The content of a NWG note may be any thought, suggestion, etc.
    related to the HOST software or other aspect of the network.
    Notes are encouraged to be timely rather than polished.
    Philosophical positions without examples or other specifics,
    specific suggestions or implementation techniques without
    introductory or background explication, and explicit questions
    without any attempted answers are all acceptable.  The minimum
    length for a NWG note is one sentence.
    #+END_QUOTE
    Somehow, the idea resonates with me a lot. And so, let's try and see what happens.
*** How?
    I like Emacs. So after some research I decided to try [[https://ox-hugo.scripter.co/][ox-hugo]] for the simplest possible solution.
** DONE Parrhasius                                                  :ruby:go:
   CLOSED: [2020-01-06 Mon 17:38]
   :PROPERTIES:
   :EXPORT_FILE_NAME: parrhasius
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :featured_image "/images/parrhasius.webp"
   :END:
   One day I found myself scrolling through [[https://4chan.org/wg][internet best source of wallpapers]] and found myself wishing for something to mass-download a whole thread.
   Without a proper google search even, decided to write something on my own. Presto! A new project.
*** Part 1: Good beginnings
    The basic part is easy enough: use [[https://nokogiri.org/][Nokogiri]] in Ruby to parse site HTML, extract image URLs:
    #+BEGIN_SRC ruby
      html = Nokogiri(open(link).read)

      html.search('a')
        .select { |l|
        l.children.size == 1 &&
          l.children.first.to_s.match(/.*(jpg|png)$/)
      }
    #+END_SRC
    and then download them:
    #+BEGIN_SRC ruby
      img_link = link.attributes['href']
      File.write(
        SecureRandom.uuid + ext(img_link.value),
        open('https:' + img_link.value).read
      )
    #+END_SRC
*** Part 2: Bad hashing
    Soon enough, the first problem becomes apparent: lots of these images are duplicates of one another.
    Long story short, it's best to use some image hashing algorithm that takes similarities into account - as some might be duplicates, but transformed (e.g. converted from jpeg to png back and forth, cropped, resized...).

    Initially, I tried to simply "normalize" images by converting to lowest quality level and then hashing resulting binary. Not only was this approach not very accurate, but very CPU-intensive. Even when using [[https://github.com/ruby-concurrency/concurrent-ruby][concurrent-ruby]] and all CPUs available, hashing was the slowest part of the process.
    My second attempt used pure Golang and tried to hash image using histograms. This too, wasn't very accurate - but a lot faster.

    After more research I stumbled upon [[http://www.hackerfactor.com/blog/?/archives/529-Kind-of-Like-That.html][DHash]].
    Since I already had a support for calling Go I decided to use an implementation in this language: https://github.com/devedge/imagehash.
    #+BEGIN_SRC go
      src, err := imagehash.OpenImg(filename)
      if err != nil {
        return "", err
      }
      hash, err := imagehash.Dhash(src, 8)

      if err != nil {
        return "", err
      }
      return hex.EncodeToString(hash), nil
    #+END_SRC
*** Part 3: Ugly glue
    The ugliest part was calling external binary to calculate the hash. At one point I stumbled upon [[https://gist.github.com/schweigert/385cd8e2267140674b6c4818d8f0c373][Gist describing Ruby-Go glue]]. This turned out to be a pretty simple task, with only one caveat: error handling.

    Golang convention is to pass possible error as 2nd return value:
    #+BEGIN_SRC go
      type cstring *C.char

      //export ExtHash
      func ExtHash(filename cstring) (cstring, cstring)
    #+END_SRC

    This proves to be quite hard to decipher from Ruby unless you read [[https://github.com/ffi/ffi][FFI docs]] quite closely. If you don't, the following Ruby definition allows calling and checking for errors properly:
    #+BEGIN_SRC ruby
      class ExtHashReturn < FFI::Struct
        layout :value, :string,
               :error, :string
      end

      attach_function :ExtHash, [:string], ExtHashReturn.by_value
    #+END_SRC
*** Part 4: Bonus
    Having the images downloaded and deduplicated, only one thing remains: browsing through them. This requires 2 things: having thumbnails generated and some simple GUI to show them (and link to original).
    Former is easy with [[https://github.com/minimagick/minimagick][minimagick]]:
    #+BEGIN_SRC ruby
      image = MiniMagick::Image.open(f.realpath)
      image.resize '256x256'
      image.write([dest, f.basename].join('/'))
    #+END_SRC
    and the latter can be quite simple with [[http://neptunian.github.io/react-photo-gallery/][React Photo Gallery]], [[https://github.com/CassetteRocks/react-infinite-scroller#readme][React Infinite Scroller]] and [[https://react-simple-img.now.sh/][React Simple Img]]. This part still requires some cleanup (and maybe Redux), so I'll delay posting code until then.

   Source code: https://github.com/scoiatael/parrhasius\\
** DONE Idea: matrix log analyzer :monitoring:
   CLOSED: [2020-03-07 Mon 12:58]
   :PROPERTIES:
   :EXPORT_FILE_NAME: matrix-logs
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :featured_image "https://raw.githubusercontent.com/wiki/akinomyoga/cxxmatrix/images/cxxmatrix-version01sA.gif"
   :END:
Recently I came upon an interesting project: [[https://github.com/akinomyoga/cxxmatrix][cxxmatrix]]. It creates matrix-like animations
in your terminal. This prompted a train of thought: it's neat, but in the movie
it had a purpose. It was used like a ~tail -f /var/log/messages~: a real-time
glimpse into what's currently happening inside the Matrix.

Can we somehow make it a /useful/ reality? My latest PITA has been exactly that:
looking into trails of logs in order to find patterns.

My debug flow consists more or less of:
- Looking for new exceptions in tracker (like Sentry),
- Reacting to alerts based on pre-defined checks,
- Using metrics (Grafana, Influxdb) to understand current situation,
- Grepping through logs on affected hosts to see more details of the situation,
The last step is one that could possibly benefit from a big improvement.
*** Idea
Ok, so what would be the solution?

Something that ingests logs much like ~tail~ or ~grep~: through ~stdin~ or text file.
It then goes through them in real time and learns patterns, while outputting
current state.
I'd envision it consisting of 2 parts:
- known patterns (list, along with simple statistics),
- output buffer (essentially parsed and colorized input based on patterns)
*** Algorithm
Inputs is a stream of logs.
Output is, for each log line, currently known patterns and whether this line
matches any pattern.

State update should reasonably keep amount of used memory bounded by some upper
constant, so it has to make a decision which log lines should be kept to try and
match with future ones. For a good starting point, it could be pre-seeded with
some known patterns by operator.

A pattern could be represented as a "string of strings", with wildcards in
between. Each wildcard should match exactly one character. This way we can
quickly compare string to a pattern seeing how many characters match.

Current state is a list of patterns, along with hit count for each of them. We
should also keep a total number of lines seen in order to implement pattern
eviction.
*** Possible problems
- Multi-line logs. Certain loggers output multiple lines for one problem (e.g.
  stacktraces). Our implementation could coalesce logs based on timestamp -
  meaning we'd need to make some assumptions about input.
- Output speed. In case of reading from file (or being fed one via ~cat~ or
  ~grep~) it's possible that output would go too fast for a human to analyze. To
  solve this use-case we could either implement:
  - slowing down input based on timestamps,
  - paging - ~less~ style,
  - non-interactive output - first analyze and spit out patterns, later allow
    operator to decide which patterns should be ignored. This requires us to
    make decisions in a deterministic fashion instead of using ~rand~ to help
    eviction strategy.
*** Usability notes
Two distinct use-cases: online and offline. Online one would be easier to
implement first.

Essentially a TUI, ~htop~-like.

Nice to have: state save and load, in order to later start with pre-seeded patterns.

Requires deterministic eviction strategy (maybe we could make a decision based
on ~rand~ seed being current input hash?).

Nice to have: simple charts for patterns [[https://github.com/mkaz/termgraph][termgraph]]-style.
*** Implementation notes
**** Language
I'd envision Rust. Python/Ruby lose on the requirement to be speedy (real-time
ideally). Erlang/Elixir doesn't really work for CLIs. Go would be a nice choice,
but I shudder at the thought of implementing even semi-complex algorithm using
~interface{}~.
*** Alternatives and inspirations
- [[https://goaccess.io][goaccess]] - neat, looks like it uses a know set of patterns to apply to text,
- termgraph, cxxmatrix, as mentioned,
- [[https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html][grok]] - my previous go-to tool for understanding logs,
- various closed-source ones - I believe each service has their own way of doing
  it. I kinda enjoyed [[https://docs.datadoghq.com/logs/processing/processors/?tab=ui#grok-parser][Datadog]] in that matter.
- [[https://github.com/google/mtail][mtail]] - a solution I looked into to solve exactly that problem. Sadly requires
  upfront definition of parsing rules.
** TODO statsd                                                :go:monitoring:
   :PROPERTIES:
   :EXPORT_FILE_NAME: statsd
   :END:
   Source code: https://github.com/scoiatael/statsd and https://github.com/scoiatael/tracing\\
** TODO dotfiles                                              :config:fish:emacs:
   :PROPERTIES:
   :EXPORT_FILE_NAME: dotfiles
   :END:
   Source code: https://github.com/scoiatael/dotfiles\\
** TODO Jarvis                                                      :clojure:
   :PROPERTIES:
   :EXPORT_FILE_NAME: jarvis
   :END:
   Source code: https://github.com/scoiatael/jarvis and https://github.com/scoiatael/masters_thesis\\
** TODO archai                                  :go:cassandra:event_sourcing:
   :PROPERTIES:
   :EXPORT_FILE_NAME: archai
   :END:
   Source code: https://github.com/scoiatael/archai\\
** TODO colorls-rs :rust:
   :PROPERTIES:
   :EXPORT_FILE_NAME: colorls_rs
   :END:
   Source code: https://github.com/scoiatael/colorls-rs\\
** TODO resume                                                         :clojure:
   :PROPERTIES:
   :EXPORT_FILE_NAME: resume
   :END:
   Source code: https://github.com/scoiatael/resume\\
   Site: https://resume.czaplin.ski\\
